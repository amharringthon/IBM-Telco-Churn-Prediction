# ğŸ“Š IBM Telco Customer Churn Prediction â€“ Data Science Project

This project aims to predict customer churn using various machine learning models and optimization techniques. By analyzing customer demographics, account information, and service usage, the goal is to identify patterns that indicate the likelihood of a customer leaving the company.

ğŸ”— **Live Project Notebook on GitHub:**  
[IBM-Telco-Churn-Prediction](https://github.com/amharringthon/IBM-Telco-Churn-Prediction)

---

## ğŸ“ Project Structure

```
ğŸ”¹ dataset/                # Contains the IBM Telco dataset  
ğŸ”¹ notebook.ipynb          # Main Jupyter notebook with full analysis  
ğŸ”¹ model/                  # Folder containing the exported final model  
ğŸ”¹ README.md               # Project overview and documentation  
```

---

## ğŸ” Dataset

The dataset is based on IBM Telco's customer base and includes:

- Demographic information (e.g., gender, senior citizen)  
- Services subscribed (e.g., internet, tech support)  
- Payment and contract details  
- Customer Lifetime Value (CLTV)  
- Churn status  

â„¹ï¸ The original dataset is publicly available on Kaggle. A copy is included in this repository for convenience.

---

## âœ¨ Project Workflow

1. **Data Cleaning**  
   - Handled missing values and removed redundant or non-informative features.

2. **Exploratory Data Analysis (EDA)**  
   - Visualized churn patterns and key relationships between features.

3. **Preprocessing**  
   - Applied label encoding, one-hot encoding, and MinMax scaling.

4. **Model Training**  
   - Trained multiple classifiers: Logistic Regression, Random Forest, XGBoost, and LightGBM.

5. **Class Imbalance Handling**  
   - Used SMOTE to address class imbalance and improve recall.

6. **Model Evaluation**  
   - Compared performance using Accuracy, Precision, Recall, F1-Score, and ROC-AUC.

7. **Hyperparameter Tuning**  
   - Performed optimization using Optuna to fine-tune LightGBM with SMOTE.

8. **Model Exporting**  
   - Saved the best model using `joblib` for production use.

---

## ğŸ§  Final Model â€“ Optimized LightGBM with SMOTE

| Metric              | Value     |
|---------------------|-----------|
| Accuracy            | 85.22%    |
| F1-Score            | 85.73%    |
| ROC-AUC             | 0.93      |
| Cross-Validation    | 84.10%    |
| Recall (Churn)      | 87%       |

âœ… The model showed **strong performance**, especially in **recall**, which is key for early churn detection.

---

## ğŸŒŸ Business Impact

Deploying this model in production would allow businesses to:

- ğŸŒŸ **Proactively identify at-risk customers**  
- ğŸ’° **Reduce churn-related revenue loss**  
- ğŸ±ï¸ **Create personalized retention strategies based on customer profiles and lifetime value**

---

## ğŸ› ï¸ Tools & Technologies

- Python (Pandas, NumPy, Scikit-learn)  
- LightGBM, XGBoost, Random Forest  
- SMOTE (Imbalanced-learn)  
- Optuna (for hyperparameter tuning)  
- Seaborn & Matplotlib (visualization)  
- Jupyter Notebook

---

## ğŸ“Œ How to Use

1. Clone the repository:
   ```bash
   git clone https://github.com/amharringthon/IBM-Telco-Churn-Prediction.git
   ```

2. Navigate to the project folder and run the notebook:
   ```bash
   jupyter notebook
   ```

3. To test the model:
   ```python
   import joblib
   model = joblib.load("model/final_lgbm_model.pkl")
   prediction = model.predict(new_data)
   ```
